{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now using cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 9\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "batch_size = 96\n",
    "num_epochs = 99\n",
    "context_len = 128\n",
    "initial_lr = 0.001\n",
    "data_path = \"./data/chinese-poetry/唐诗\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"now using\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE: 3482\n",
      "data_size 1287\n"
     ]
    }
   ],
   "source": [
    "with open(\"poems.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    poems = json.load(f)\n",
    "\n",
    "with open(\"vocab.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word_to_index = json.load(f)\n",
    "\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "\n",
    "print(\"VOCAB_SIZE:\", vocab_size)\n",
    "print(\"data_size\", len(poems))\n",
    "\n",
    "\n",
    "# 将句子转换为列表形式，并添加结束符\n",
    "poems = [list(poem) + [\"<EOP>\"] for poem in poems]\n",
    "index_tensors = {\n",
    "    word: torch.LongTensor([word_to_index[word]]) for word in word_to_index\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(poem):\n",
    "\n",
    "    inputs = [index_tensors[poem[i - 1]] for i in range(1, len(poem))]\n",
    "    outputs = [index_tensors[poem[i]] for i in range(1, len(poem))]\n",
    "\n",
    "    # 将输入和输出列表合并为张量\n",
    "    encoded_inputs = torch.cat(inputs)\n",
    "    encoded_outputs = torch.cat(outputs)\n",
    "\n",
    "    return encoded_inputs, encoded_outputs\n",
    "\n",
    "\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, poems, transform=None):\n",
    "        self.poems = poems\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.poems)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        poem = self.poems[index]\n",
    "        input_data, output_data = generate_sample(poem)\n",
    "        if self.transform:\n",
    "            input_data = self.transform(input_data)\n",
    "        return input_data, output_data\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    inputs, outputs = zip(*batch)\n",
    "    # 统一长度以进行批处理\n",
    "    padded_inputs = nn.utils.rnn.pad_sequence(\n",
    "        inputs, batch_first=True, padding_value=word_to_index[\"<START>\"]\n",
    "    )\n",
    "    padded_outputs = nn.utils.rnn.pad_sequence(\n",
    "        outputs, batch_first=True, padding_value=word_to_index[\"<START>\"]\n",
    "    )\n",
    "    return padded_inputs, padded_outputs\n",
    "\n",
    "\n",
    "dataset = PoetryDataset(poems)\n",
    "data_loader = DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../multi-head.png)\n",
    "\n",
    "![](../self-attention.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads=4, mask=False):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        assert embed_size % num_heads == 0, \"Embedding size 必须是 heads 的整数倍\"\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "        # 计算所有 heads 的 query, key 和 value\n",
    "        self.query_projection = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.key_projection = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.value_projection = nn.Linear(embed_size, embed_size, bias=False)\n",
    "\n",
    "        # 在 multi-head self-attention 操作后应用\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, embed_size = x.size()\n",
    "\n",
    "        # 将输入 x 分别通过线性层投影到 query, key 和 value 向量\n",
    "        # 只用三次 k×k 矩阵乘法就能实现 multi-head 功能\n",
    "        # 唯一需要的额外操作是将生成的 output vector 重新按块排序\n",
    "        queries = self.query_projection(x).view(\n",
    "            batch_size, seq_length, self.num_heads, self.head_dim\n",
    "        )\n",
    "        keys = self.key_projection(x).view(\n",
    "            batch_size, seq_length, self.num_heads, self.head_dim\n",
    "        )\n",
    "        values = self.value_projection(x).view(\n",
    "            batch_size, seq_length, self.num_heads, self.head_dim\n",
    "        )\n",
    "\n",
    "        # print(queries.size())\n",
    "        # torch.Size([1, 5, 4, 64])\n",
    "\n",
    "        # 将 tensor 重新排列，以适应 multi-head attention\n",
    "        queries = (\n",
    "            queries.transpose(1, 2)\n",
    "            .reshape(batch_size * self.num_heads, seq_length, self.head_dim)\n",
    "        )\n",
    "        keys = (\n",
    "            keys.transpose(1, 2)\n",
    "            .reshape(batch_size * self.num_heads, seq_length, self.head_dim)\n",
    "        )\n",
    "        values = (\n",
    "            values.transpose(1, 2)\n",
    "            .reshape(batch_size * self.num_heads, seq_length, self.head_dim)\n",
    "        )\n",
    "\n",
    "        # print(queries.size())\n",
    "        # torch.Size([4, 5, 64])\n",
    "\n",
    "        # 计算 Scaled dot-product attention 点积相关度矩阵\n",
    "        dot_product = torch.bmm(queries, keys.transpose(1, 2))\n",
    "        # print(dot_product.size())\n",
    "        # torch.Size([4, 5, 5])\n",
    "\n",
    "        # softmax 函数对非常大的输入值敏感。\n",
    "        # 这些 input 会梯度消失，学习变慢甚至完全停止。\n",
    "        # 由于点积的平均值随着嵌入维度 k 的增加而增大\n",
    "        # 因此点积送到 softmax 之前进行缩放有助于缓解这个问题。\n",
    "        scaled_dot_product = dot_product / (self.embed_size**0.5)\n",
    "\n",
    "        # 如果启用了 mask，则对未来的 token 进行屏蔽\n",
    "        if self.mask:\n",
    "            # torch.triu(..., diagonal=1)：保留上三角部分\n",
    "            # 指定 diagonal=1 表示从第一个对角线开始（即排除主对角线），其余部分设为零\n",
    "            mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool()\n",
    "            mask = mask.to(device)\n",
    "            scaled_dot_product.masked_fill_(mask, float(\"-inf\"))\n",
    "\n",
    "        attention = F.softmax(scaled_dot_product, dim=2)\n",
    "\n",
    "        # 将 self-attention 应用于 values\n",
    "        # print(torch.bmm(attention, values).size())\n",
    "        # torch.Size([4, 5, 64])\n",
    "        out = torch.bmm(attention, values).reshape(\n",
    "            batch_size, self.num_heads, seq_length, self.head_dim\n",
    "        )\n",
    "        # print(out.size())\n",
    "        # torch.Size([1, 4, 5, 64])\n",
    "        out = (\n",
    "            out.transpose(1, 2)\n",
    "            .reshape(batch_size, seq_length, self.embed_size)\n",
    "        )\n",
    "        # print(out.size())\n",
    "        # torch.Size([1, 5, 256])\n",
    "        return self.fc_out(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../transformer-block.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, mask=False):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.attention = SelfAttention(embed_size, num_heads=num_heads, mask=mask)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention 和残差连接\n",
    "        attended = self.attention(x)\n",
    "        x = self.norm1(attended + x)\n",
    "\n",
    "        # 前馈神经网络和残差连接\n",
    "        fedforward = self.ff(x)\n",
    "        return self.norm2(fedforward + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../transformer-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_size,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        context_len,\n",
    "        num_tokens,\n",
    "        num_classes,\n",
    "        mask=False,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.token_emb = nn.Embedding(num_tokens, embed_size)\n",
    "        self.pos_emb = nn.Embedding(context_len, embed_size)\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            *[TransformerBlock(embed_size, num_heads, mask) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, num_classes)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length = x.size()\n",
    "\n",
    "        # 生成 token 嵌入\n",
    "        tokens = self.token_emb(x)\n",
    "        # 生成位置嵌入\n",
    "        positions = torch.arange(seq_length).to(x.device)\n",
    "        # print(positions.size())\n",
    "        # torch.Size([5])\n",
    "        # print(self.pos_emb(positions).size())\n",
    "        # torch.Size([5, 256])\n",
    "\n",
    "        positions = self.pos_emb(positions).expand(batch_size, seq_length, -1)\n",
    "        # print(positions.size())\n",
    "        # torch.Size([1, 5, 256])\n",
    "        \n",
    "\n",
    "        # 将 token 嵌入和位置嵌入相加\n",
    "        x = tokens + positions\n",
    "\n",
    "        # 通过所有 Transformer 层\n",
    "        x = self.layers(x)\n",
    "\n",
    "        # 最后映射到类概率\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, num_epochs, device, optimizer, criterion, scheduler, model_name):\n",
    "    log_dict = {\n",
    "        \"train_loss_per_epoch\": [],\n",
    "        \"train_perplexity_per_epoch\": [],\n",
    "        \"model_name\": model_name,\n",
    "    }\n",
    "    start_time = time.time()\n",
    "    model = model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1:03d}/{num_epochs:03d} | Current Learning Rate: {current_lr:.6f}\"\n",
    "        )\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            if not batch_idx % 5:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch + 1:03d}/{num_epochs:03d} | Batch {batch_idx:04d}/{len(data_loader):04d} | Loss: {loss:.6f}\"\n",
    "                )\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader.dataset)\n",
    "        scheduler.step(avg_loss)\n",
    "        perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "        log_dict[\"train_loss_per_epoch\"].append(avg_loss)\n",
    "        log_dict[\"train_perplexity_per_epoch\"].append(perplexity)\n",
    "\n",
    "        print(f\"Time elapsed: {(time.time() - start_time) / 60:.2f} min\")\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{model_name}_model_state_dict.pth\")\n",
    "    print(f\"Total Training Time: {(time.time() - start_time)/ 60:.2f} min\")\n",
    "    return log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Transformer                              [1, 1, 3482]              --\n",
       "├─Embedding: 1-1                         [1, 1, 256]               891,392\n",
       "├─Embedding: 1-2                         [1, 256]                  32,768\n",
       "├─Sequential: 1-3                        [1, 1, 256]               --\n",
       "│    └─TransformerBlock: 2-1             [1, 1, 256]               --\n",
       "│    │    └─SelfAttention: 3-1           [1, 1, 256]               --\n",
       "│    │    │    └─Linear: 4-1             [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-2             [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-3             [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-4             [1, 1, 256]               65,792\n",
       "│    │    └─LayerNorm: 3-2               [1, 1, 256]               512\n",
       "│    │    └─Sequential: 3-3              [1, 1, 256]               --\n",
       "│    │    │    └─Linear: 4-5             [1, 1, 1024]              263,168\n",
       "│    │    │    └─ReLU: 4-6               [1, 1, 1024]              --\n",
       "│    │    │    └─Linear: 4-7             [1, 1, 256]               262,400\n",
       "│    │    └─LayerNorm: 3-4               [1, 1, 256]               512\n",
       "│    └─TransformerBlock: 2-2             [1, 1, 256]               --\n",
       "│    │    └─SelfAttention: 3-5           [1, 1, 256]               --\n",
       "│    │    │    └─Linear: 4-8             [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-9             [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-10            [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-11            [1, 1, 256]               65,792\n",
       "│    │    └─LayerNorm: 3-6               [1, 1, 256]               512\n",
       "│    │    └─Sequential: 3-7              [1, 1, 256]               --\n",
       "│    │    │    └─Linear: 4-12            [1, 1, 1024]              263,168\n",
       "│    │    │    └─ReLU: 4-13              [1, 1, 1024]              --\n",
       "│    │    │    └─Linear: 4-14            [1, 1, 256]               262,400\n",
       "│    │    └─LayerNorm: 3-8               [1, 1, 256]               512\n",
       "│    └─TransformerBlock: 2-3             [1, 1, 256]               --\n",
       "│    │    └─SelfAttention: 3-9           [1, 1, 256]               --\n",
       "│    │    │    └─Linear: 4-15            [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-16            [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-17            [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-18            [1, 1, 256]               65,792\n",
       "│    │    └─LayerNorm: 3-10              [1, 1, 256]               512\n",
       "│    │    └─Sequential: 3-11             [1, 1, 256]               --\n",
       "│    │    │    └─Linear: 4-19            [1, 1, 1024]              263,168\n",
       "│    │    │    └─ReLU: 4-20              [1, 1, 1024]              --\n",
       "│    │    │    └─Linear: 4-21            [1, 1, 256]               262,400\n",
       "│    │    └─LayerNorm: 3-12              [1, 1, 256]               512\n",
       "│    └─TransformerBlock: 2-4             [1, 1, 256]               --\n",
       "│    │    └─SelfAttention: 3-13          [1, 1, 256]               --\n",
       "│    │    │    └─Linear: 4-22            [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-23            [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-24            [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-25            [1, 1, 256]               65,792\n",
       "│    │    └─LayerNorm: 3-14              [1, 1, 256]               512\n",
       "│    │    └─Sequential: 3-15             [1, 1, 256]               --\n",
       "│    │    │    └─Linear: 4-26            [1, 1, 1024]              263,168\n",
       "│    │    │    └─ReLU: 4-27              [1, 1, 1024]              --\n",
       "│    │    │    └─Linear: 4-28            [1, 1, 256]               262,400\n",
       "│    │    └─LayerNorm: 3-16              [1, 1, 256]               512\n",
       "│    └─TransformerBlock: 2-5             [1, 1, 256]               --\n",
       "│    │    └─SelfAttention: 3-17          [1, 1, 256]               --\n",
       "│    │    │    └─Linear: 4-29            [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-30            [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-31            [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-32            [1, 1, 256]               65,792\n",
       "│    │    └─LayerNorm: 3-18              [1, 1, 256]               512\n",
       "│    │    └─Sequential: 3-19             [1, 1, 256]               --\n",
       "│    │    │    └─Linear: 4-33            [1, 1, 1024]              263,168\n",
       "│    │    │    └─ReLU: 4-34              [1, 1, 1024]              --\n",
       "│    │    │    └─Linear: 4-35            [1, 1, 256]               262,400\n",
       "│    │    └─LayerNorm: 3-20              [1, 1, 256]               512\n",
       "│    └─TransformerBlock: 2-6             [1, 1, 256]               --\n",
       "│    │    └─SelfAttention: 3-21          [1, 1, 256]               --\n",
       "│    │    │    └─Linear: 4-36            [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-37            [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-38            [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-39            [1, 1, 256]               65,792\n",
       "│    │    └─LayerNorm: 3-22              [1, 1, 256]               512\n",
       "│    │    └─Sequential: 3-23             [1, 1, 256]               --\n",
       "│    │    │    └─Linear: 4-40            [1, 1, 1024]              263,168\n",
       "│    │    │    └─ReLU: 4-41              [1, 1, 1024]              --\n",
       "│    │    │    └─Linear: 4-42            [1, 1, 256]               262,400\n",
       "│    │    └─LayerNorm: 3-24              [1, 1, 256]               512\n",
       "│    └─TransformerBlock: 2-7             [1, 1, 256]               --\n",
       "│    │    └─SelfAttention: 3-25          [1, 1, 256]               --\n",
       "│    │    │    └─Linear: 4-43            [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-44            [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-45            [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-46            [1, 1, 256]               65,792\n",
       "│    │    └─LayerNorm: 3-26              [1, 1, 256]               512\n",
       "│    │    └─Sequential: 3-27             [1, 1, 256]               --\n",
       "│    │    │    └─Linear: 4-47            [1, 1, 1024]              263,168\n",
       "│    │    │    └─ReLU: 4-48              [1, 1, 1024]              --\n",
       "│    │    │    └─Linear: 4-49            [1, 1, 256]               262,400\n",
       "│    │    └─LayerNorm: 3-28              [1, 1, 256]               512\n",
       "│    └─TransformerBlock: 2-8             [1, 1, 256]               --\n",
       "│    │    └─SelfAttention: 3-29          [1, 1, 256]               --\n",
       "│    │    │    └─Linear: 4-50            [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-51            [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-52            [1, 1, 256]               65,536\n",
       "│    │    │    └─Linear: 4-53            [1, 1, 256]               65,792\n",
       "│    │    └─LayerNorm: 3-30              [1, 1, 256]               512\n",
       "│    │    └─Sequential: 3-31             [1, 1, 256]               --\n",
       "│    │    │    └─Linear: 4-54            [1, 1, 1024]              263,168\n",
       "│    │    │    └─ReLU: 4-55              [1, 1, 1024]              --\n",
       "│    │    │    └─Linear: 4-56            [1, 1, 256]               262,400\n",
       "│    │    └─LayerNorm: 3-32              [1, 1, 256]               512\n",
       "├─Linear: 1-4                            [1, 1, 3482]              894,874\n",
       "==========================================================================================\n",
       "Total params: 8,130,970\n",
       "Trainable params: 8,130,970\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 8.13\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.21\n",
       "Params size (MB): 32.52\n",
       "Estimated Total Size (MB): 32.74\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_training_stats(log_dict):\n",
    "    model_name = log_dict[\"model_name\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(log_dict[\"train_loss_per_epoch\"], label=\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"{model_name}_Training Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(log_dict[\"train_perplexity_per_epoch\"], label=\"Training Perplexity\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.grid(True)\n",
    "    # plt.yscale(\"log\")\n",
    "    plt.title(f\"{model_name}_Training Perplexity\")\n",
    "    plt.savefig(f\"{model_name}_training_stats.svg\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "mask = True\n",
    "model = Transformer(\n",
    "    embed_size=256,\n",
    "    num_heads=8,\n",
    "    num_layers=8,\n",
    "    context_len=context_len,\n",
    "    num_tokens=vocab_size,\n",
    "    num_classes=vocab_size,\n",
    "    mask=True,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=9, verbose=True\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[\"<START>\"])\n",
    "# log_dict = train(\n",
    "#     model, data_loader, num_epochs, device, optimizer, criterion, scheduler, \"transformer\"\n",
    "# )\n",
    "# plot_training_stats(log_dict)\n",
    "model.load_state_dict(torch.load(\"model_state_dict.pth\"))\n",
    "model.to(device)\n",
    "\n",
    "inputs = torch.tensor([[1]]).to(device) \n",
    "summary(model, input_data=inputs, depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./training_stats.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高楼入青天，下有白玉堂。明月看欲堕，当窗悬清辉。遥夜一美人，罗衣霑秋霜。含情弄柔瑟，弹作陌上桑。\n",
      "长安一片月，万户捣衣声。秋风吹不尽，总是玉关情。何日平胡虏。良人罢远征。\n",
      "长安一片月，万户捣衣声。秋风吹不尽，总是玉关情。\n",
      "月皎昭阳殿，霜清长信宫。天行乘玉辇，翠羽破手人。更有留情处，承恩乐未穷。谁怜团扇妾，独坐怨秋风。\n",
      "月落宫车动，门里桃李西。人闲故人幸，此地犹不歇。\n",
      "月色不可扫，客愁不可道。玉雨悠悠人，流萤飞百草。目览我欲暮，关路紫烟沈。谁怜广陵国，独有采菱奏。\n",
      "月皎昭阳殿，冰歌荒野中。长鸿似空舞，归来忧不移。\n",
      "月皎昭阳殿，霜清长信宫。霜清且一杯，天乐尽见山。更有留情处，金茎松下尘。谁怜成道者，独坐怨秋风。\n",
      "月色不可扫，客愁不可道。客散黄金液，攀取最长枝。昔老士人起，终日暮景迟。谁因见玉此，问言长数里。\n",
      "月皎昭阳殿，霜清长信宫。天行懵道远，归来不可识。长杨收宿鸟，承恩乐未穷。谁怜团扇妾，独坐怨秋霜。\n",
      "月皎昭阳殿，霜清长信宫。天行乘玉辇，飞燕与君同。\n",
      "月殿移椒壁，天花代遐服。阴成青草木，云路尽见云。\n",
      "月落雪满动，至君降九华。何不如传箭，归来成荒淫。\n",
      "海鸟知天风，窜身鲁门东。临觞不能饮，矫翼思凌空。钟鼓不为乐，烟霜谁与同。\n",
      "海鸟知天风，窜身鲁门东。临觞不能饮，矫翼思凌空。\n",
      "海水昔飞动，三龙纷战争。钟山危波澜，倾侧骇奔鲸。黄旗一扫荡，割壤开吴京。六代更霸王，遗迹见都城。至今秦淮间，礼乐桑榆春。地扇邹鲁学，诗腾颜谢名。\n",
      "海水昔飞动，三龙纷战争。钟山危波澜，倾侧骇奔鲸。黄旗一扫荡，割壤开吴京。六代英豪气，遗迹见都城。至今秦淮间，礼乐秀群英。地扇邹鲁学，诗腾颜谢名。\n",
      "海鸟知天风，窜身鲁门东。临觞不能饮，矫翼思凌空。钟鼓不为乐，烟霜谁与同。\n",
      "海鸟知天风，窜身鲁门东。临觞不能饮，矫翼思凌空。钟鼓不为乐，烟霜谁与同。\n",
      "海水不满眼，观涛难称心。即知蓬莱石，却是巨鼇簪。送尔游华顶，令余发舃吟。却笑严湍上，问言中者心。\n",
      "海鸟知天风，窜身鲁门东。临觞不能饮，矫翼思凌空。钟鼓不为乐，烟霜谁与同。\n",
      "海鸟知天风，窜身鲁门东。临觞不能饮，矫翼思凌空。钟鼓不为乐，烟霜谁与同。\n",
      "海客乘天风，将船远行役。譬如云中鸟，一去无踪迹。\n",
      "烟: 0.4328\n",
      "静: 0.2852\n",
      "露: 0.2820\n",
      "风\n",
      "纪: 0.9946\n",
      "卧: 0.0035\n",
      "风: 0.0019\n",
      "风烟\n",
      "南: 0.9959\n",
      "寒: 0.0033\n",
      "谁: 0.0008\n",
      "风烟纪\n",
      "城: 0.9998\n",
      "平: 0.0002\n",
      "岭: 0.0001\n",
      "风烟纪南\n",
      "，: 1.0000\n",
      "知: 0.0000\n",
      "前: 0.0000\n",
      "风烟纪南城\n",
      "尘: 0.9997\n",
      "霜: 0.0002\n",
      "池: 0.0001\n",
      "风烟纪南城，\n",
      "土: 0.9975\n",
      "埃: 0.0022\n",
      "浓: 0.0004\n",
      "风烟纪南城，尘\n",
      "荆: 0.9967\n",
      "阳: 0.0019\n",
      "湖: 0.0014\n",
      "风烟纪南城，尘土\n",
      "门: 0.9999\n",
      "江: 0.0000\n",
      "风: 0.0000\n",
      "风烟纪南城，尘土荆\n",
      "路: 0.9996\n",
      "楼: 0.0002\n",
      "山: 0.0002\n",
      "风烟纪南城，尘土荆门\n",
      "。: 1.0000\n",
      "平: 0.0000\n",
      "谢: 0.0000\n",
      "风烟纪南城，尘土荆门路\n",
      "天: 0.9242\n",
      "走: 0.0474\n",
      "高: 0.0285\n",
      "风烟纪南城，尘土荆门路。\n",
      "寒: 0.9856\n",
      "威: 0.0117\n",
      "无: 0.0027\n",
      "风烟纪南城，尘土荆门路。天\n",
      "猎: 0.6199\n",
      "多: 0.3787\n",
      "休: 0.0014\n",
      "风烟纪南城，尘土荆门路。天寒\n",
      "兽: 0.9783\n",
      "骑: 0.0171\n",
      "红: 0.0046\n",
      "风烟纪南城，尘土荆门路。天寒猎\n",
      "者: 0.9953\n",
      "色: 0.0028\n",
      "情: 0.0018\n",
      "风烟纪南城，尘土荆门路。天寒猎兽\n",
      "，: 1.0000\n",
      "下: 0.0000\n",
      "薄: 0.0000\n",
      "风烟纪南城，尘土荆门路。天寒猎兽者\n",
      "走: 0.9977\n",
      "独: 0.0018\n",
      "洛: 0.0005\n",
      "风烟纪南城，尘土荆门路。天寒猎兽者，\n",
      "上: 0.9999\n",
      "海: 0.0000\n",
      "江: 0.0000\n",
      "风烟纪南城，尘土荆门路。天寒猎兽者，走\n",
      "樊: 0.9557\n",
      "改: 0.0303\n",
      "鼓: 0.0140\n",
      "风烟纪南城，尘土荆门路。天寒猎兽者，走上\n",
      "姬: 0.9992\n",
      "马: 0.0004\n",
      "衣: 0.0004\n",
      "风烟纪南城，尘土荆门路。天寒猎兽者，走上樊\n",
      "墓: 0.9973\n",
      "花: 0.0014\n",
      "重: 0.0013\n",
      "风烟纪南城，尘土荆门路。天寒猎兽者，走上樊姬\n",
      "。: 1.0000\n",
      "遥: 0.0000\n",
      "万: 0.0000\n",
      "风烟纪南城，尘土荆门路。天寒猎兽者，走上樊姬墓\n",
      "<EOP>: 1.0000\n",
      "行: 0.0000\n",
      "荒: 0.0000\n",
      "风烟纪南城，尘土荆门路。天寒猎兽者，走上樊姬墓。\n"
     ]
    }
   ],
   "source": [
    "def generate_text(start_word, top_k=1, temperature=0.7, log=False):\n",
    "    generated_text = \"\"\n",
    "    index_tensors_list = []\n",
    "    for word in start_word:\n",
    "        index_tensors_list.append(index_tensors[word].unsqueeze(0))\n",
    "        generated_text += word\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(context_len - len(generated_text)):\n",
    "            input_tensor = torch.tensor(index_tensors_list).unsqueeze(0).to(device)\n",
    "            # print(input_tensor.size())\n",
    "            # torch.Size([1, 5])\n",
    "            output = model(input_tensor.to(device))\n",
    "\n",
    "            last_word = output[:, -1, :]\n",
    "            last_word = last_word.view(-1)\n",
    "\n",
    "            # 调整温度\n",
    "            # softmax 函数倾向于增强输入向量中最大值的影响\n",
    "            scaled_logits = last_word / temperature\n",
    "            probabilities = F.softmax(scaled_logits, dim=-1)\n",
    "\n",
    "            probabilities, top_indices = probabilities.data.topk(top_k)\n",
    "            top_words = [index_to_word[index.item()] for index in top_indices]\n",
    "            probabilities = probabilities / torch.sum(probabilities)\n",
    "\n",
    "            probabilities_np = probabilities.cpu().numpy()\n",
    "            indices_np = top_indices.cpu().numpy()\n",
    "            if log:\n",
    "                for word, prob in zip(top_words, probabilities_np):\n",
    "                    print(f\"{word}: {prob:.4f}\")\n",
    "\n",
    "            selected_index = np.random.choice(indices_np, p=probabilities_np)\n",
    "\n",
    "            next_word = index_to_word[selected_index]\n",
    "            if next_word == \"<EOP>\":\n",
    "                break\n",
    "            if log:\n",
    "                print(generated_text)\n",
    "            index_tensors_list.append(index_tensors[next_word])\n",
    "            generated_text += next_word\n",
    "\n",
    "    return generated_text.strip()\n",
    "\n",
    "\n",
    "print(generate_text(\"高楼入青天\", top_k=1))\n",
    "print(generate_text(\"长安一片月\", top_k=3))\n",
    "print(generate_text(\"长安一片月\", top_k=3, temperature=1.2))\n",
    "for i in range(10):\n",
    "    print(generate_text(\"月\", top_k=20, temperature=1.1))\n",
    "for i in range(10):\n",
    "    print(generate_text(\"海\", top_k=3))\n",
    "print(generate_text(\"风\", top_k=3, log=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不见一杯酒，相君不异归。\n",
      "如何红烬落，却是隐深游。\n",
      "见客如先觉，衣离不象宽。\n",
      "一惑巧言子，独自有仁回。\n",
      "面起在人重，独酌陶永夕。\n",
      "\n",
      "深居俯夹城，春去映呼香。\n",
      "度衰犹自舞，在此两般花。\n",
      "学以亭亭月，微寒云外愁。\n",
      "习朝晒红焰，春西北堂宫。\n"
     ]
    }
   ],
   "source": [
    "def generate_acrostic(start_word, top_k=1, temperature=0.7, log=False):\n",
    "    generated_text = \"\"\n",
    "    words = []\n",
    "    for word in start_word:\n",
    "        words += [word]\n",
    "    index_tensors_list = []\n",
    "    index_tensors_list.append(index_tensors[words[0]].unsqueeze(0))\n",
    "    generated_text += words[0]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        ind = 1\n",
    "        for _ in range(context_len - len(generated_text)):\n",
    "            input = torch.tensor(index_tensors_list).unsqueeze(0).to(device)\n",
    "            output = model(input)\n",
    "            last_word = output[:, -1, :]\n",
    "            last_word = last_word.view(-1)\n",
    "\n",
    "            # 调整温度\n",
    "            # softmax 函数倾向于增强输入向量中最大值的影响\n",
    "            scaled_logits = last_word / temperature\n",
    "            probabilities = F.softmax(scaled_logits, dim=-1)\n",
    "\n",
    "            probabilities, top_indices = probabilities.data.topk(top_k)\n",
    "            top_words = [index_to_word[index.item()] for index in top_indices]\n",
    "            probabilities = probabilities / torch.sum(probabilities)\n",
    "\n",
    "            probabilities_np = probabilities.cpu().detach().numpy()\n",
    "            indices_np = top_indices.cpu().detach().numpy()\n",
    "            if log:\n",
    "                for word, prob in zip(top_words, probabilities_np):\n",
    "                    print(f\"{word}: {prob:.4f}\")\n",
    "\n",
    "            selected_index = np.random.choice(indices_np, p=probabilities_np)\n",
    "\n",
    "            next_word = index_to_word[selected_index]\n",
    "            if next_word == \"<EOP>\":\n",
    "                break\n",
    "            generated_text += next_word\n",
    "\n",
    "            # 如果遇到句号感叹号等，把藏头的词作为下一个句的输入\n",
    "            if next_word in [\"。\"]:\n",
    "                # 如果生成的诗歌已经包含全部藏头的词，则结束\n",
    "                if ind == len(start_word):\n",
    "                    break\n",
    "                # 把藏头的词作为输入，预测下一个词\n",
    "                index_tensors_list.append(index_tensors[next_word])\n",
    "                index_tensors_list.append(index_tensors[words[ind]])\n",
    "                generated_text = generated_text + '\\n' + words[ind]\n",
    "                ind += 1\n",
    "            else:\n",
    "                index_tensors_list.append(index_tensors[next_word])\n",
    "\n",
    "            if log:\n",
    "                print(generated_text)\n",
    "\n",
    "    return generated_text.strip()\n",
    "\n",
    "print(generate_acrostic(\"不如见一面\", top_k=20, temperature=1.2))\n",
    "print()\n",
    "print(generate_acrostic(\"深度学习\", top_k=20, temperature=1.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_resnet_models():\n",
    "    resnet_models = {\n",
    "        \"Transformer_2_heads\": (\n",
    "            Transformer(\n",
    "                embed_size=128,\n",
    "                num_heads=2,\n",
    "                num_layers=8,\n",
    "                context_len=context_len,\n",
    "                num_tokens=vocab_size,\n",
    "                num_classes=vocab_size,\n",
    "                mask=True,\n",
    "            ).to(device),\n",
    "            50,\n",
    "            0.001,\n",
    "        ),\n",
    "        \"Transformer_4_heads\": (\n",
    "            Transformer(\n",
    "                embed_size=128,\n",
    "                num_heads=4,\n",
    "                num_layers=8,\n",
    "                context_len=context_len,\n",
    "                num_tokens=vocab_size,\n",
    "                num_classes=vocab_size,\n",
    "                mask=True,\n",
    "            ).to(device),\n",
    "            50,\n",
    "            0.001,\n",
    "        ),\n",
    "        \"Transformer_8_heads\": (\n",
    "            Transformer(\n",
    "                embed_size=128,\n",
    "                num_heads=8,\n",
    "                num_layers=8,\n",
    "                context_len=context_len,\n",
    "                num_tokens=vocab_size,\n",
    "                num_classes=vocab_size,\n",
    "                mask=True,\n",
    "            ).to(device),\n",
    "            50,\n",
    "            0.001,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    log_dicts = []\n",
    "\n",
    "    for model_name, (model, num_epochs, initial_lr) in resnet_models.items():\n",
    "        print(\n",
    "            f\"Training {model_name} for {num_epochs} epochs with initial learning rate {initial_lr}...\"\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=initial_lr, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.5, patience=9, verbose=True\n",
    "        )\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[\"<START>\"])\n",
    "        log_dict = train(\n",
    "            model, data_loader, num_epochs, device, optimizer, criterion, scheduler, model_name\n",
    "        )\n",
    "\n",
    "        log_dicts.append(log_dict)\n",
    "        model.load_state_dict(torch.load(f\"{model_name}_model_state_dict.pth\"))\n",
    "        plot_training_stats(log_dict)\n",
    "\n",
    "    return log_dicts\n",
    "\n",
    "\n",
    "def plot_compare(log_dicts):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    color_list = [\"#6495ED\", \"#4EEE94\", \"#EEC900\", \"#FF6347\", \"#BA55D3\", \"#00808C\"]\n",
    "    ind_color = 0\n",
    "    for log_dict in log_dicts:\n",
    "        train_loss_per_epoch = log_dict[\"train_loss_per_epoch\"]\n",
    "        model_name = log_dict[\"model_name\"]\n",
    "        train_perplexity_per_epoch = log_dict[\"train_perplexity_per_epoch\"]\n",
    "\n",
    "        axs[0].plot(\n",
    "            np.arange(1, len(train_perplexity_per_epoch) + 1),\n",
    "            train_perplexity_per_epoch,\n",
    "            \".--\",\n",
    "            color=color_list[ind_color],\n",
    "            label=f\"{model_name}\",\n",
    "        )\n",
    "\n",
    "        axs[1].plot(\n",
    "            np.arange(1, len(train_loss_per_epoch) + 1),\n",
    "            train_loss_per_epoch,\n",
    "            \".--\",\n",
    "            color=color_list[ind_color],\n",
    "            label=f\"{model_name}\",\n",
    "        )\n",
    "        ind_color += 1\n",
    "\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"train_perplexity_per_epoch\")\n",
    "    axs[0].set_title(f\"Training Perplexity\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"train_loss_per_epoch\")\n",
    "    axs[1].set_title(f\"Training Loss\")\n",
    "    axs[0].legend(loc=\"best\")\n",
    "    axs[1].legend(loc=\"best\")\n",
    "    axs[0].grid(True)\n",
    "    axs[1].grid(True)\n",
    "    fig.savefig(\"training_performance.svg\", format=\"svg\")\n",
    "    fig.show()\n",
    "\n",
    "# log_dicts = train_all_resnet_models()\n",
    "# plot_compare(log_dicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
