{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now using cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 9\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 99\n",
    "context_len = 128\n",
    "initial_lr = 0.001\n",
    "data_path = \"./data/chinese-poetry/唐诗\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"now using\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6558, 0.3020, 0.4799, 0.7774, 0.9180],\n",
      "         [0.9310, 0.2604, 0.9534, 0.3804, 0.4104],\n",
      "         [0.9510, 0.5686, 0.1381, 0.2069, 0.5139]]])\n",
      "tensor([[[2.1985, 1.8192, 1.4942],\n",
      "         [1.8192, 2.1568, 1.4548],\n",
      "         [1.4942, 1.4548, 1.5537]]])\n",
      "tensor([[[0.4590, 0.3141, 0.2269],\n",
      "         [0.3230, 0.4527, 0.2243],\n",
      "         [0.3308, 0.3180, 0.3511]]])\n",
      "tensor([[[0.8092, 0.3494, 0.5511, 0.5232, 0.6669],\n",
      "         [0.8466, 0.3430, 0.6176, 0.4697, 0.5976],\n",
      "         [0.8470, 0.3824, 0.5105, 0.4508, 0.6147]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 3, 5)\n",
    "print(x)\n",
    "raw_weights = torch.bmm(x, x.transpose(1, 2))\n",
    "print(raw_weights)\n",
    "weights = F.softmax(raw_weights, dim=2)\n",
    "print(weights)\n",
    "y = torch.bmm(weights, x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE: 3482\n",
      "data_size 1287\n"
     ]
    }
   ],
   "source": [
    "with open(\"poems.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    poems = json.load(f)\n",
    "\n",
    "with open(\"vocab.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word_to_index = json.load(f)\n",
    "\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "\n",
    "print(\"VOCAB_SIZE:\", vocab_size)\n",
    "print(\"data_size\", len(poems))\n",
    "\n",
    "\n",
    "# 将句子转换为列表形式，并添加结束符\n",
    "poems = [list(poem) + [\"<EOP>\"] for poem in poems]\n",
    "index_tensors = {\n",
    "    word: torch.LongTensor([word_to_index[word]]) for word in word_to_index\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(poem):\n",
    "\n",
    "    inputs = [index_tensors[poem[i - 1]] for i in range(1, len(poem))]\n",
    "    outputs = [index_tensors[poem[i]] for i in range(1, len(poem))]\n",
    "\n",
    "    # 将输入和输出列表合并为张量\n",
    "    encoded_inputs = torch.cat(inputs)\n",
    "    encoded_outputs = torch.cat(outputs)\n",
    "\n",
    "    return encoded_inputs, encoded_outputs\n",
    "\n",
    "\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, poems, transform=None):\n",
    "        self.poems = poems\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.poems)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        poem = self.poems[index]\n",
    "        input_data, output_data = generate_sample(poem)\n",
    "        if self.transform:\n",
    "            input_data = self.transform(input_data)\n",
    "        return input_data, output_data\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    inputs, outputs = zip(*batch)\n",
    "    # 统一长度以进行批处理\n",
    "    padded_inputs = nn.utils.rnn.pad_sequence(\n",
    "        inputs, batch_first=True, padding_value=word_to_index[\"<START>\"]\n",
    "    )\n",
    "    padded_outputs = nn.utils.rnn.pad_sequence(\n",
    "        outputs, batch_first=True, padding_value=word_to_index[\"<START>\"]\n",
    "    )\n",
    "    return padded_inputs, padded_outputs\n",
    "\n",
    "\n",
    "dataset = PoetryDataset(poems)\n",
    "data_loader = DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../multi-head.png)\n",
    "\n",
    "![](../self-attention.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads=4, mask=False):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        assert embed_size % num_heads == 0, \"Embedding size 必须是 heads 的整数倍\"\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "        # 计算所有 heads 的 query, key 和 value\n",
    "        self.query_projection = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.key_projection = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.value_projection = nn.Linear(embed_size, embed_size, bias=False)\n",
    "\n",
    "        # 在 multi-head self-attention 操作后应用\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, embed_size = x.size()\n",
    "\n",
    "        # 将输入 x 分别通过线性层投影到 query, key 和 value 向量\n",
    "        # 只用三次 k×k 矩阵乘法就能实现 multi-head 功能\n",
    "        # 唯一需要的额外操作是将生成的 output vector 重新按块排序\n",
    "        queries = self.query_projection(x).view(\n",
    "            batch_size, seq_length, self.num_heads, self.head_dim\n",
    "        )\n",
    "        keys = self.key_projection(x).view(\n",
    "            batch_size, seq_length, self.num_heads, self.head_dim\n",
    "        )\n",
    "        values = self.value_projection(x).view(\n",
    "            batch_size, seq_length, self.num_heads, self.head_dim\n",
    "        )\n",
    "\n",
    "        # print(queries.size())\n",
    "        # torch.Size([1, 5, 4, 64])\n",
    "\n",
    "        # 将 tensor 重新排列，以适应 multi-head attention\n",
    "        queries = (\n",
    "            queries.transpose(1, 2)\n",
    "            .reshape(batch_size * self.num_heads, seq_length, self.head_dim)\n",
    "        )\n",
    "        keys = (\n",
    "            keys.transpose(1, 2)\n",
    "            .reshape(batch_size * self.num_heads, seq_length, self.head_dim)\n",
    "        )\n",
    "        values = (\n",
    "            values.transpose(1, 2)\n",
    "            .reshape(batch_size * self.num_heads, seq_length, self.head_dim)\n",
    "        )\n",
    "\n",
    "        # print(queries.size())\n",
    "        # torch.Size([4, 5, 64])\n",
    "\n",
    "        # 计算 Scaled dot-product attention 点积相关度矩阵\n",
    "        dot_product = torch.bmm(queries, keys.transpose(1, 2))\n",
    "        # print(dot_product.size())\n",
    "        # torch.Size([4, 5, 5])\n",
    "\n",
    "        # softmax 函数对非常大的输入值敏感。\n",
    "        # 这些 input 会梯度消失，学习变慢甚至完全停止。\n",
    "        # 由于点积的平均值随着嵌入维度 k 的增加而增大\n",
    "        # 因此点积送到 softmax 之前进行缩放有助于缓解这个问题。\n",
    "        scaled_dot_product = dot_product / (self.embed_size**0.5)\n",
    "\n",
    "        # 如果启用了 mask，则对未来的 token 进行屏蔽\n",
    "        if self.mask:\n",
    "            # torch.triu(..., diagonal=1)：保留上三角部分\n",
    "            # 指定 diagonal=1 表示从第一个对角线开始（即排除主对角线），其余部分设为零\n",
    "            mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool()\n",
    "            mask = mask.to(device)\n",
    "            scaled_dot_product.masked_fill_(mask, float(\"-inf\"))\n",
    "\n",
    "        attention = F.softmax(scaled_dot_product, dim=2)\n",
    "\n",
    "        # 将 self-attention 应用于 values\n",
    "        # print(torch.bmm(attention, values).size())\n",
    "        # torch.Size([4, 5, 64])\n",
    "        out = torch.bmm(attention, values).reshape(\n",
    "            batch_size, self.num_heads, seq_length, self.head_dim\n",
    "        )\n",
    "        # print(out.size())\n",
    "        # torch.Size([1, 4, 5, 64])\n",
    "        out = (\n",
    "            out.transpose(1, 2)\n",
    "            .reshape(batch_size, seq_length, self.embed_size)\n",
    "        )\n",
    "        # print(out.size())\n",
    "        # torch.Size([1, 5, 256])\n",
    "        return self.fc_out(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../transformer-block.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, mask=False):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.attention = SelfAttention(embed_size, num_heads=num_heads, mask=mask)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention 和残差连接\n",
    "        attended = self.attention(x)\n",
    "        x = self.norm1(attended + x)\n",
    "\n",
    "        # 前馈神经网络和残差连接\n",
    "        fedforward = self.ff(x)\n",
    "        return self.norm2(fedforward + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../transformer-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_size,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        seq_length,\n",
    "        num_tokens,\n",
    "        num_classes,\n",
    "        mask=False,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.token_emb = nn.Embedding(num_tokens, embed_size)\n",
    "        self.pos_emb = nn.Embedding(seq_length, embed_size)\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            *[TransformerBlock(embed_size, num_heads, mask) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length = x.size()\n",
    "\n",
    "        # 生成 token 嵌入\n",
    "        tokens = self.token_emb(x)\n",
    "\n",
    "        # 生成位置嵌入\n",
    "        positions = torch.arange(seq_length).to(x.device)\n",
    "        # print(positions.size())\n",
    "        # torch.Size([5])\n",
    "        # print(self.pos_emb(positions).size())\n",
    "        # torch.Size([5, 256])\n",
    "\n",
    "        positions = self.pos_emb(positions).reshape(batch_size, seq_length, -1)\n",
    "        # print(positions.size())\n",
    "        # torch.Size([1, 5, 256])\n",
    "\n",
    "        # 将 token 嵌入和位置嵌入相加\n",
    "        x = tokens + positions\n",
    "\n",
    "        # 通过所有 Transformer 层\n",
    "        x = self.layers(x)\n",
    "\n",
    "        # 最后映射到类概率\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, num_epochs, device, optimizer, criterion, scheduler):\n",
    "    log_dict = {\n",
    "        \"train_loss_per_epoch\": [],\n",
    "        \"train_perplexity_per_epoch\": [],\n",
    "    }\n",
    "    start_time = time.time()\n",
    "    model = model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1:03d}/{num_epochs:03d} | Current Learning Rate: {current_lr:.6f}\"\n",
    "        )\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            if not batch_idx % 5:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch + 1:03d}/{num_epochs:03d} | Batch {batch_idx:04d}/{len(data_loader):04d} | Loss: {loss:.6f}\"\n",
    "                )\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader.dataset)\n",
    "        scheduler.step(avg_loss)\n",
    "        perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "        log_dict[\"train_loss_per_epoch\"].append(avg_loss)\n",
    "        log_dict[\"train_perplexity_per_epoch\"].append(perplexity)\n",
    "\n",
    "        print(f\"Time elapsed: {(time.time() - start_time) / 60:.2f} min\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"model_state_dict.pth\")\n",
    "    print(f\"Total Training Time: {(time.time() - start_time)/ 60:.2f} min\")\n",
    "    return log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Transformer                              [1, 1, 3482]              --\n",
       "├─Embedding: 1-1                         [1, 1, 256]               891,392\n",
       "├─Embedding: 1-2                         [1, 256]                  32,768\n",
       "├─Sequential: 1-3                        [1, 1, 256]               --\n",
       "│    └─TransformerBlock: 2-1             [1, 1, 256]               --\n",
       "│    │    └─SelfAttention: 3-1           [1, 1, 256]               262,400\n",
       "│    │    └─LayerNorm: 3-2               [1, 1, 256]               512\n",
       "│    │    └─Sequential: 3-3              [1, 1, 256]               525,568\n",
       "│    │    └─LayerNorm: 3-4               [1, 1, 256]               512\n",
       "│    └─TransformerBlock: 2-2             [1, 1, 256]               --\n",
       "│    │    └─SelfAttention: 3-5           [1, 1, 256]               262,400\n",
       "│    │    └─LayerNorm: 3-6               [1, 1, 256]               512\n",
       "│    │    └─Sequential: 3-7              [1, 1, 256]               525,568\n",
       "│    │    └─LayerNorm: 3-8               [1, 1, 256]               512\n",
       "│    └─TransformerBlock: 2-3             [1, 1, 256]               --\n",
       "│    │    └─SelfAttention: 3-9           [1, 1, 256]               262,400\n",
       "│    │    └─LayerNorm: 3-10              [1, 1, 256]               512\n",
       "│    │    └─Sequential: 3-11             [1, 1, 256]               525,568\n",
       "│    │    └─LayerNorm: 3-12              [1, 1, 256]               512\n",
       "│    └─TransformerBlock: 2-4             [1, 1, 256]               --\n",
       "│    │    └─SelfAttention: 3-13          [1, 1, 256]               262,400\n",
       "│    │    └─LayerNorm: 3-14              [1, 1, 256]               512\n",
       "│    │    └─Sequential: 3-15             [1, 1, 256]               525,568\n",
       "│    │    └─LayerNorm: 3-16              [1, 1, 256]               512\n",
       "│    └─TransformerBlock: 2-5             [1, 1, 256]               --\n",
       "│    │    └─SelfAttention: 3-17          [1, 1, 256]               262,400\n",
       "│    │    └─LayerNorm: 3-18              [1, 1, 256]               512\n",
       "│    │    └─Sequential: 3-19             [1, 1, 256]               525,568\n",
       "│    │    └─LayerNorm: 3-20              [1, 1, 256]               512\n",
       "│    └─TransformerBlock: 2-6             [1, 1, 256]               --\n",
       "│    │    └─SelfAttention: 3-21          [1, 1, 256]               262,400\n",
       "│    │    └─LayerNorm: 3-22              [1, 1, 256]               512\n",
       "│    │    └─Sequential: 3-23             [1, 1, 256]               525,568\n",
       "│    │    └─LayerNorm: 3-24              [1, 1, 256]               512\n",
       "│    └─TransformerBlock: 2-7             [1, 1, 256]               --\n",
       "│    │    └─SelfAttention: 3-25          [1, 1, 256]               262,400\n",
       "│    │    └─LayerNorm: 3-26              [1, 1, 256]               512\n",
       "│    │    └─Sequential: 3-27             [1, 1, 256]               525,568\n",
       "│    │    └─LayerNorm: 3-28              [1, 1, 256]               512\n",
       "│    └─TransformerBlock: 2-8             [1, 1, 256]               --\n",
       "│    │    └─SelfAttention: 3-29          [1, 1, 256]               262,400\n",
       "│    │    └─LayerNorm: 3-30              [1, 1, 256]               512\n",
       "│    │    └─Sequential: 3-31             [1, 1, 256]               525,568\n",
       "│    │    └─LayerNorm: 3-32              [1, 1, 256]               512\n",
       "├─Linear: 1-4                            [1, 1, 3482]              894,874\n",
       "==========================================================================================\n",
       "Total params: 8,130,970\n",
       "Trainable params: 8,130,970\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 8.13\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.21\n",
       "Params size (MB): 32.52\n",
       "Estimated Total Size (MB): 32.74\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_training_stats(log_dict):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(log_dict[\"train_loss_per_epoch\"], label=\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(log_dict[\"train_perplexity_per_epoch\"], label=\"Training Perplexity\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.grid(True)\n",
    "    # plt.yscale(\"log\")\n",
    "    plt.title(\"Training Perplexity\")\n",
    "    plt.savefig(\"training_stats.svg\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "mask = True\n",
    "model = Transformer(\n",
    "    embed_size=256,\n",
    "    num_heads=8,\n",
    "    num_layers=8,\n",
    "    seq_length=context_len,\n",
    "    num_tokens=vocab_size,\n",
    "    num_classes=vocab_size,\n",
    "    mask=True,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=9, verbose=True\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[\"<START>\"])\n",
    "# log_dict = train(\n",
    "#     model, data_loader, num_epochs, device, optimizer, criterion, scheduler\n",
    "# )\n",
    "# plot_training_stats(log_dict)\n",
    "model.load_state_dict(torch.load(\"model_state_dict.pth\"))\n",
    "model.to(device)\n",
    "\n",
    "inputs = torch.tensor([[1]]).to(device) \n",
    "summary(model, input_data=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./training_stats.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "长安一片月，万户捣衣声。秋风吹不尽，总是玉关情。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "月落宫车动，风方见秋空。路远天乐府，人尚想龙颜。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "月殿移椒壁，天花代舜华。唯余采香径，一如坠壶中。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1, 25])\n",
      "torch.Size([1, 26])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1, 28])\n",
      "torch.Size([1, 29])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1, 31])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 33])\n",
      "torch.Size([1, 34])\n",
      "torch.Size([1, 35])\n",
      "torch.Size([1, 36])\n",
      "torch.Size([1, 37])\n",
      "torch.Size([1, 38])\n",
      "torch.Size([1, 39])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1, 41])\n",
      "torch.Size([1, 42])\n",
      "torch.Size([1, 43])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1, 45])\n",
      "torch.Size([1, 46])\n",
      "torch.Size([1, 47])\n",
      "torch.Size([1, 48])\n",
      "月落宫车动，风凄仪杖闲。路唯瞻凤老，人尚想龙颜。玉童收羽骑，乘云流芳歇。圣情悲风态，谁能慕成丝。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "月皎昭阳殿，霜清长信宫。天行悲清夜，归来能鹤鸣。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1, 25])\n",
      "torch.Size([1, 26])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1, 28])\n",
      "torch.Size([1, 29])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1, 31])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 33])\n",
      "torch.Size([1, 34])\n",
      "torch.Size([1, 35])\n",
      "torch.Size([1, 36])\n",
      "月落宫车动，风凄凉金华。路唯瞻军远，人尚想龙山。御宇方无事，乘云怆意哀。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1, 25])\n",
      "torch.Size([1, 26])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1, 28])\n",
      "torch.Size([1, 29])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1, 31])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 33])\n",
      "torch.Size([1, 34])\n",
      "torch.Size([1, 35])\n",
      "torch.Size([1, 36])\n",
      "torch.Size([1, 37])\n",
      "torch.Size([1, 38])\n",
      "torch.Size([1, 39])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1, 41])\n",
      "torch.Size([1, 42])\n",
      "torch.Size([1, 43])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1, 45])\n",
      "torch.Size([1, 46])\n",
      "torch.Size([1, 47])\n",
      "torch.Size([1, 48])\n",
      "月色不可扫，客愁不可道。玉露生秋衣，烟雪耻容繁。日暮醉酒见，别离成久游。此地为吊客，寄谢远相逢。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1, 25])\n",
      "torch.Size([1, 26])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1, 28])\n",
      "torch.Size([1, 29])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1, 31])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 33])\n",
      "torch.Size([1, 34])\n",
      "torch.Size([1, 35])\n",
      "torch.Size([1, 36])\n",
      "torch.Size([1, 37])\n",
      "torch.Size([1, 38])\n",
      "torch.Size([1, 39])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1, 41])\n",
      "torch.Size([1, 42])\n",
      "torch.Size([1, 43])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1, 45])\n",
      "torch.Size([1, 46])\n",
      "torch.Size([1, 47])\n",
      "torch.Size([1, 48])\n",
      "月露发光彩，此时方有余。尽日西来时，天外颁楼流。隔水旧如此，志存到应相。岁晏天地尽，独宿粲成丝。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1, 25])\n",
      "torch.Size([1, 26])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1, 28])\n",
      "torch.Size([1, 29])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1, 31])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 33])\n",
      "torch.Size([1, 34])\n",
      "torch.Size([1, 35])\n",
      "torch.Size([1, 36])\n",
      "torch.Size([1, 37])\n",
      "torch.Size([1, 38])\n",
      "torch.Size([1, 39])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1, 41])\n",
      "torch.Size([1, 42])\n",
      "torch.Size([1, 43])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1, 45])\n",
      "torch.Size([1, 46])\n",
      "torch.Size([1, 47])\n",
      "torch.Size([1, 48])\n",
      "月色不可解，此地若浮秋。挥鞭访江雪，偏伤周浪软。正西望九宇，志存到临流。相知照欢意，相随艑上皇。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1, 25])\n",
      "torch.Size([1, 26])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1, 28])\n",
      "torch.Size([1, 29])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1, 31])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 33])\n",
      "torch.Size([1, 34])\n",
      "torch.Size([1, 35])\n",
      "torch.Size([1, 36])\n",
      "torch.Size([1, 37])\n",
      "torch.Size([1, 38])\n",
      "torch.Size([1, 39])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1, 41])\n",
      "torch.Size([1, 42])\n",
      "torch.Size([1, 43])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1, 45])\n",
      "torch.Size([1, 46])\n",
      "torch.Size([1, 47])\n",
      "torch.Size([1, 48])\n",
      "月色不可扫，客愁不可道。玉露生秋衣，流萤飞百草。日月终销雪，天地皆卷柳。吾柳新安道，投景复追游。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1, 25])\n",
      "torch.Size([1, 26])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1, 28])\n",
      "torch.Size([1, 29])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1, 31])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 33])\n",
      "torch.Size([1, 34])\n",
      "torch.Size([1, 35])\n",
      "torch.Size([1, 36])\n",
      "torch.Size([1, 37])\n",
      "torch.Size([1, 38])\n",
      "torch.Size([1, 39])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1, 41])\n",
      "torch.Size([1, 42])\n",
      "torch.Size([1, 43])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1, 45])\n",
      "torch.Size([1, 46])\n",
      "torch.Size([1, 47])\n",
      "torch.Size([1, 48])\n",
      "月色不可扫，客愁不可道。玉露生秋衣，高殿锁藏辉。日月容火起，天地同枯荣。只愁群岫开，清凉润举目。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1, 25])\n",
      "torch.Size([1, 26])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1, 28])\n",
      "torch.Size([1, 29])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1, 31])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 33])\n",
      "torch.Size([1, 34])\n",
      "torch.Size([1, 35])\n",
      "torch.Size([1, 36])\n",
      "torch.Size([1, 37])\n",
      "torch.Size([1, 38])\n",
      "torch.Size([1, 39])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1, 41])\n",
      "torch.Size([1, 42])\n",
      "torch.Size([1, 43])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1, 45])\n",
      "torch.Size([1, 46])\n",
      "torch.Size([1, 47])\n",
      "torch.Size([1, 48])\n",
      "torch.Size([1, 49])\n",
      "torch.Size([1, 50])\n",
      "torch.Size([1, 51])\n",
      "torch.Size([1, 52])\n",
      "torch.Size([1, 53])\n",
      "torch.Size([1, 54])\n",
      "torch.Size([1, 55])\n",
      "torch.Size([1, 56])\n",
      "torch.Size([1, 57])\n",
      "torch.Size([1, 58])\n",
      "torch.Size([1, 59])\n",
      "torch.Size([1, 60])\n",
      "torch.Size([1, 61])\n",
      "torch.Size([1, 62])\n",
      "torch.Size([1, 63])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 66])\n",
      "torch.Size([1, 67])\n",
      "torch.Size([1, 68])\n",
      "torch.Size([1, 69])\n",
      "torch.Size([1, 70])\n",
      "torch.Size([1, 71])\n",
      "torch.Size([1, 72])\n",
      "海水昔飞动，三龙纷战争。钟山危波澜，挥翰动龙吟。黄旗一扫荡，割壤开吴京。人生勤将军，浮云知音者。至今秦淮间，共乘双飞鸾。地扇邹鲁学，相顾共登城。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1, 25])\n",
      "torch.Size([1, 26])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1, 28])\n",
      "torch.Size([1, 29])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1, 31])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 33])\n",
      "torch.Size([1, 34])\n",
      "torch.Size([1, 35])\n",
      "torch.Size([1, 36])\n",
      "海鸟知天风，窜身鲁门东。临觞不能饮，矫翼思凌空。钟鼓不为乐，烟霜谁与同。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1, 25])\n",
      "torch.Size([1, 26])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1, 28])\n",
      "torch.Size([1, 29])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1, 31])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 33])\n",
      "torch.Size([1, 34])\n",
      "torch.Size([1, 35])\n",
      "torch.Size([1, 36])\n",
      "torch.Size([1, 37])\n",
      "torch.Size([1, 38])\n",
      "torch.Size([1, 39])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1, 41])\n",
      "torch.Size([1, 42])\n",
      "torch.Size([1, 43])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1, 45])\n",
      "torch.Size([1, 46])\n",
      "torch.Size([1, 47])\n",
      "torch.Size([1, 48])\n",
      "torch.Size([1, 49])\n",
      "torch.Size([1, 50])\n",
      "torch.Size([1, 51])\n",
      "torch.Size([1, 52])\n",
      "torch.Size([1, 53])\n",
      "torch.Size([1, 54])\n",
      "torch.Size([1, 55])\n",
      "torch.Size([1, 56])\n",
      "torch.Size([1, 57])\n",
      "torch.Size([1, 58])\n",
      "torch.Size([1, 59])\n",
      "torch.Size([1, 60])\n",
      "torch.Size([1, 61])\n",
      "torch.Size([1, 62])\n",
      "torch.Size([1, 63])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 66])\n",
      "torch.Size([1, 67])\n",
      "torch.Size([1, 68])\n",
      "torch.Size([1, 69])\n",
      "torch.Size([1, 70])\n",
      "torch.Size([1, 71])\n",
      "torch.Size([1, 72])\n",
      "海水昔飞动，三龙纷战争。钟山危波澜，倾侧骇奔鲸。黄旗一扫荡，割壤开吴京。六代更霸王，遗迹见都城。至今秦淮间，共乘双飞翻。地扇邹鲁学，相忆在寥廓。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1, 25])\n",
      "torch.Size([1, 26])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1, 28])\n",
      "torch.Size([1, 29])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1, 31])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 33])\n",
      "torch.Size([1, 34])\n",
      "torch.Size([1, 35])\n",
      "torch.Size([1, 36])\n",
      "torch.Size([1, 37])\n",
      "torch.Size([1, 38])\n",
      "torch.Size([1, 39])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1, 41])\n",
      "torch.Size([1, 42])\n",
      "torch.Size([1, 43])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1, 45])\n",
      "torch.Size([1, 46])\n",
      "torch.Size([1, 47])\n",
      "torch.Size([1, 48])\n",
      "torch.Size([1, 49])\n",
      "torch.Size([1, 50])\n",
      "torch.Size([1, 51])\n",
      "torch.Size([1, 52])\n",
      "torch.Size([1, 53])\n",
      "torch.Size([1, 54])\n",
      "torch.Size([1, 55])\n",
      "torch.Size([1, 56])\n",
      "torch.Size([1, 57])\n",
      "torch.Size([1, 58])\n",
      "torch.Size([1, 59])\n",
      "torch.Size([1, 60])\n",
      "torch.Size([1, 61])\n",
      "torch.Size([1, 62])\n",
      "torch.Size([1, 63])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 66])\n",
      "torch.Size([1, 67])\n",
      "torch.Size([1, 68])\n",
      "torch.Size([1, 69])\n",
      "torch.Size([1, 70])\n",
      "torch.Size([1, 71])\n",
      "torch.Size([1, 72])\n",
      "海水昔飞动，三龙纷战争。钟山危波澜，倾侧骇奔鲸。黄旗一扫荡，割壤开吴京。六代英豪气，遗迹见都城。至今秦淮间，礼乐秀群英。地扇邹鲁学，未肯委严霜。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "海客乘天风，将船远行役。譬如云中鸟，一去无踪迹。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1, 25])\n",
      "torch.Size([1, 26])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1, 28])\n",
      "torch.Size([1, 29])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1, 31])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 33])\n",
      "torch.Size([1, 34])\n",
      "torch.Size([1, 35])\n",
      "torch.Size([1, 36])\n",
      "torch.Size([1, 37])\n",
      "torch.Size([1, 38])\n",
      "torch.Size([1, 39])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1, 41])\n",
      "torch.Size([1, 42])\n",
      "torch.Size([1, 43])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1, 45])\n",
      "torch.Size([1, 46])\n",
      "torch.Size([1, 47])\n",
      "torch.Size([1, 48])\n",
      "torch.Size([1, 49])\n",
      "torch.Size([1, 50])\n",
      "torch.Size([1, 51])\n",
      "torch.Size([1, 52])\n",
      "torch.Size([1, 53])\n",
      "torch.Size([1, 54])\n",
      "torch.Size([1, 55])\n",
      "torch.Size([1, 56])\n",
      "torch.Size([1, 57])\n",
      "torch.Size([1, 58])\n",
      "torch.Size([1, 59])\n",
      "torch.Size([1, 60])\n",
      "torch.Size([1, 61])\n",
      "torch.Size([1, 62])\n",
      "torch.Size([1, 63])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 66])\n",
      "torch.Size([1, 67])\n",
      "torch.Size([1, 68])\n",
      "torch.Size([1, 69])\n",
      "torch.Size([1, 70])\n",
      "torch.Size([1, 71])\n",
      "torch.Size([1, 72])\n",
      "海水昔飞动，三龙纷战争。钟山危波澜，倾侧骇奔鲸。黄旗一扫荡，割壤开吴京。六代更霸王，遗迹见都城。至今秦淮间，共一在人愁。地扇邹鲁学，宁亲归汝坟。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1, 25])\n",
      "torch.Size([1, 26])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1, 28])\n",
      "torch.Size([1, 29])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1, 31])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 33])\n",
      "torch.Size([1, 34])\n",
      "torch.Size([1, 35])\n",
      "torch.Size([1, 36])\n",
      "海鸟知天风，窜身鲁门东。临觞不能饮，矫翼思凌空。钟鼓不为乐，烟霜谁与同。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "海客乘天风，将船远行役。譬如云中鸟，一去无踪迹。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1, 25])\n",
      "torch.Size([1, 26])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1, 28])\n",
      "torch.Size([1, 29])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1, 31])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 33])\n",
      "torch.Size([1, 34])\n",
      "torch.Size([1, 35])\n",
      "torch.Size([1, 36])\n",
      "torch.Size([1, 37])\n",
      "torch.Size([1, 38])\n",
      "torch.Size([1, 39])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1, 41])\n",
      "torch.Size([1, 42])\n",
      "torch.Size([1, 43])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1, 45])\n",
      "torch.Size([1, 46])\n",
      "torch.Size([1, 47])\n",
      "torch.Size([1, 48])\n",
      "torch.Size([1, 49])\n",
      "torch.Size([1, 50])\n",
      "torch.Size([1, 51])\n",
      "torch.Size([1, 52])\n",
      "torch.Size([1, 53])\n",
      "torch.Size([1, 54])\n",
      "torch.Size([1, 55])\n",
      "torch.Size([1, 56])\n",
      "torch.Size([1, 57])\n",
      "torch.Size([1, 58])\n",
      "torch.Size([1, 59])\n",
      "torch.Size([1, 60])\n",
      "torch.Size([1, 61])\n",
      "torch.Size([1, 62])\n",
      "torch.Size([1, 63])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 66])\n",
      "torch.Size([1, 67])\n",
      "torch.Size([1, 68])\n",
      "torch.Size([1, 69])\n",
      "torch.Size([1, 70])\n",
      "torch.Size([1, 71])\n",
      "torch.Size([1, 72])\n",
      "海水昔飞动，三龙纷战争。钟山危波澜，倾侧骇奔鲸。黄旗一扫荡，割壤开吴京。六代更霸王，浮云知音者。至今秦淮间，仙标发近边。欲得识此物，相忆在麟阁。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1, 25])\n",
      "torch.Size([1, 26])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1, 28])\n",
      "torch.Size([1, 29])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1, 31])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 33])\n",
      "torch.Size([1, 34])\n",
      "torch.Size([1, 35])\n",
      "torch.Size([1, 36])\n",
      "海鸟知天风，窜身鲁门东。临觞不能饮，矫翼思凌空。钟鼓不为乐，烟霜谁与同。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 24])\n",
      "风筝吟秋空，不假更多老。高人灵府间，律吕伴咸京。\n"
     ]
    }
   ],
   "source": [
    "def generate_text(start_word, top_k=1, temperature=0.7, log=False):\n",
    "    generated_text = \"\"\n",
    "    index_tensors_list = []\n",
    "    for word in start_word:\n",
    "        index_tensors_list.append(index_tensors[word].unsqueeze(0))\n",
    "        generated_text += word\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(context_len - len(generated_text)):\n",
    "            input_tensor = torch.tensor(index_tensors_list).unsqueeze(0).to(device)\n",
    "            # print(input_tensor.size())\n",
    "            # torch.Size([1, 5])\n",
    "            output = model(input_tensor.to(device))\n",
    "\n",
    "            last_word = output[:, -1, :]\n",
    "            last_word = last_word.view(-1)\n",
    "\n",
    "            # 调整温度\n",
    "            # softmax 函数倾向于增强输入向量中最大值的影响\n",
    "            scaled_logits = last_word / temperature\n",
    "            probabilities = F.softmax(scaled_logits, dim=-1)\n",
    "\n",
    "            probabilities, top_indices = probabilities.data.topk(top_k)\n",
    "            top_words = [index_to_word[index.item()] for index in top_indices]\n",
    "            probabilities = probabilities / torch.sum(probabilities)\n",
    "\n",
    "            probabilities_np = probabilities.cpu().numpy()\n",
    "            indices_np = top_indices.cpu().numpy()\n",
    "            if log:\n",
    "                for word, prob in zip(top_words, probabilities_np):\n",
    "                    print(f\"{word}: {prob:.4f}\")\n",
    "\n",
    "            selected_index = np.random.choice(indices_np, p=probabilities_np)\n",
    "\n",
    "            next_word = index_to_word[selected_index]\n",
    "            if next_word == \"<EOP>\":\n",
    "                break\n",
    "            if log:\n",
    "                print(generated_text)\n",
    "            index_tensors_list.append(index_tensors[next_word])\n",
    "            generated_text += next_word\n",
    "\n",
    "    return generated_text.strip()\n",
    "\n",
    "\n",
    "print(generate_text(\"长安一片月\", top_k=1))\n",
    "for i in range(10):\n",
    "    print(generate_text(\"月\", top_k=20, temperature=1.1))\n",
    "for i in range(10):\n",
    "    print(generate_text(\"海\", top_k=3))\n",
    "print(generate_text(\"风\", top_k=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不出积雨际，人间四十年。\n",
      "如何本临镜，只是栋金茎。\n",
      "见客多秦宫，牛夫不象宽。\n",
      "一惑巧言子，谁木白杨园。\n",
      "面佩动仙家，法星悬高名。\n"
     ]
    }
   ],
   "source": [
    "def generate_acrostic(start_word, top_k=1, temperature=0.7, log=False):\n",
    "    generated_text = \"\"\n",
    "    words = []\n",
    "    for word in start_word:\n",
    "        words += [word]\n",
    "    index_tensors_list = []\n",
    "    index_tensors_list.append(index_tensors[words[0]].unsqueeze(0))\n",
    "    generated_text += words[0]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        ind = 1\n",
    "        for _ in range(context_len - len(generated_text)):\n",
    "            input = torch.tensor(index_tensors_list).unsqueeze(0).to(device)\n",
    "            output = model(input)\n",
    "            last_word = output[:, -1, :]\n",
    "            last_word = last_word.view(-1)\n",
    "\n",
    "            # 调整温度\n",
    "            # softmax 函数倾向于增强输入向量中最大值的影响\n",
    "            scaled_logits = last_word / temperature\n",
    "            probabilities = F.softmax(scaled_logits, dim=-1)\n",
    "\n",
    "            probabilities, top_indices = probabilities.data.topk(top_k)\n",
    "            top_words = [index_to_word[index.item()] for index in top_indices]\n",
    "            probabilities = probabilities / torch.sum(probabilities)\n",
    "\n",
    "            probabilities_np = probabilities.cpu().detach().numpy()\n",
    "            indices_np = top_indices.cpu().detach().numpy()\n",
    "            if log:\n",
    "                for word, prob in zip(top_words, probabilities_np):\n",
    "                    print(f\"{word}: {prob:.4f}\")\n",
    "\n",
    "            selected_index = np.random.choice(indices_np, p=probabilities_np)\n",
    "\n",
    "            next_word = index_to_word[selected_index]\n",
    "            if next_word == \"<EOP>\":\n",
    "                break\n",
    "            generated_text += next_word\n",
    "\n",
    "            # 如果遇到句号感叹号等，把藏头的词作为下一个句的输入\n",
    "            if next_word in [\"。\"]:\n",
    "                # 如果生成的诗歌已经包含全部藏头的词，则结束\n",
    "                if ind == len(start_word):\n",
    "                    break\n",
    "                # 把藏头的词作为输入，预测下一个词\n",
    "                index_tensors_list.append(index_tensors[next_word])\n",
    "                index_tensors_list.append(index_tensors[words[ind]])\n",
    "                generated_text = generated_text + '\\n' + words[ind]\n",
    "                ind += 1\n",
    "            else:\n",
    "                index_tensors_list.append(index_tensors[next_word])\n",
    "\n",
    "            if log:\n",
    "                print(generated_text)\n",
    "\n",
    "    return generated_text.strip()\n",
    "\n",
    "print(generate_acrostic(\"不如见一面\", top_k=40, temperature=1.2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
