{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import opencc\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 100\n",
    "context_len = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"now using\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 3, 5)\n",
    "print(x)\n",
    "raw_weights = torch.bmm(x, x.transpose(1, 2))\n",
    "print(raw_weights)\n",
    "weights = F.softmax(raw_weights, dim=2)\n",
    "print(weights)\n",
    "y = torch.bmm(weights, x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = opencc.OpenCC(\"t2s\")\n",
    "\n",
    "\n",
    "def sentenceParse(para):\n",
    "    para = re.sub(r\"（.*?）\", \"\", para)\n",
    "    para = re.sub(r\"{.*?}\", \"\", para)\n",
    "    para = re.sub(r\"《.*?》\", \"\", para)\n",
    "    para = re.sub(r\"[\\[\\]]\", \"\", para)\n",
    "    para = \"\".join([s for s in para if s not in \"0123456789-\"])\n",
    "    para = re.sub(r\"。。\", \"。\", para)\n",
    "    para = converter.convert(para)\n",
    "    if \"𫗋\" in para or len(para) > context_len:\n",
    "        return \"\"\n",
    "    return para\n",
    "\n",
    "\n",
    "def parseRawData(author=None, constrain=None):\n",
    "    def handleJson(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        rst = []\n",
    "        for poetry in data:\n",
    "            if author and poetry.get(\"author\") != author:\n",
    "                continue\n",
    "\n",
    "            # paragraphs = poetry.get(\"paragraphs\")\n",
    "            paragraphs = poetry.get(\"content\")\n",
    "            print(paragraphs)\n",
    "\n",
    "            if any(\n",
    "                len(tr) != constrain and len(tr) != 0\n",
    "                for s in paragraphs\n",
    "                for tr in re.split(\"[，！。]\", s)\n",
    "                if constrain is not None\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            pdata = \"\".join(paragraphs)\n",
    "            pdata = sentenceParse(pdata)\n",
    "            if pdata:\n",
    "                rst.append(pdata)\n",
    "        return rst\n",
    "\n",
    "    poems = []\n",
    "    # src_path = Path(\"./data/chinese-poetry-a/全唐诗/\")\n",
    "    src_path = Path(\"./data/chinese-poetry/唐诗/\")\n",
    "\n",
    "    # for file_path in src_path.glob(\"poet.tang*\"):\n",
    "    for file_path in src_path.glob(\"data*\"):\n",
    "        poems.extend(handleJson(file_path))\n",
    "    # for file_path in src_path.glob(\"poet.song*\"):\n",
    "    # data.extend(handleJson(file_path))\n",
    "    return poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems = parseRawData(author=\"李白\")\n",
    "poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建词汇表\n",
    "word_to_index = {}\n",
    "for poem in poems:\n",
    "    for word in poem:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "word_to_index[\"<EOP>\"] = len(word_to_index)  # End Of Poem token\n",
    "word_to_index[\"<START>\"] = len(word_to_index)  # Start token\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "\n",
    "print(\"VOCAB_SIZE:\", vocab_size)\n",
    "print(\"data_size\", len(poems))\n",
    "\n",
    "\n",
    "# 将句子转换为列表形式，并添加结束符\n",
    "def sentence_to_list(sentence):\n",
    "    return list(sentence) + [\"<EOP>\"]\n",
    "\n",
    "\n",
    "poems = [sentence_to_list(poem) for poem in poems]\n",
    "\n",
    "\n",
    "# 创建单词到one-hot向量的映射\n",
    "def create_one_hot_vector(word, word_to_index):\n",
    "    return torch.LongTensor([word_to_index[word]])\n",
    "\n",
    "\n",
    "one_hot_vectors = {\n",
    "    word: create_one_hot_vector(word, word_to_index) for word in word_to_index\n",
    "}\n",
    "one_hot_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(sequence, one_hot_encoding):\n",
    "    # 打印原始序列（可选）\n",
    "    # print(sequence)\n",
    "\n",
    "    # 使用列表推导式生成输入和输出的 one-hot 编码\n",
    "    inputs = [one_hot_encoding[sequence[i - 1]] for i in range(1, len(sequence))]\n",
    "    outputs = [one_hot_encoding[sequence[i]] for i in range(1, len(sequence))]\n",
    "\n",
    "    # 将输入和输出列表合并为张量\n",
    "    encoded_inputs = torch.cat(inputs)\n",
    "    encoded_outputs = torch.cat(outputs)\n",
    "\n",
    "    return encoded_inputs, encoded_outputs\n",
    "\n",
    "\n",
    "# generate_sample(poems[0], one_hot_vectors)\n",
    "\n",
    "\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, poems, transform=None):\n",
    "        self.poems = poems\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.poems)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        poem = self.poems[index]\n",
    "        input_data, output_data = generate_sample(poem, one_hot_vectors)\n",
    "        if self.transform:\n",
    "            input_data = self.transform(input_data)\n",
    "        return input_data, output_data\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    sequences, targets = zip(*batch)\n",
    "    # 统一长度以进行批处理\n",
    "    padded_sequences = nn.utils.rnn.pad_sequence(\n",
    "        sequences, batch_first=True, padding_value=word_to_index[\"<START>\"]\n",
    "    )\n",
    "    padded_targets = nn.utils.rnn.pad_sequence(\n",
    "        targets, batch_first=True, padding_value=word_to_index[\"<START>\"]\n",
    "    )\n",
    "    return padded_sequences, padded_targets\n",
    "\n",
    "\n",
    "dataset = PoetryDataset(poems)\n",
    "data_loader = DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, k, heads=4, mask=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert k % heads == 0  # input vector size 必须是 heads 的整数倍\n",
    "        self.k, self.heads = k, heads\n",
    "\n",
    "        # Compute the queries, keys and values for all heads\n",
    "        self.tokeys = nn.Linear(k, k, bias=False)\n",
    "        self.toqueries = nn.Linear(k, k, bias=False)\n",
    "        self.tovalues = nn.Linear(k, k, bias=False)\n",
    "\n",
    "        # This will be applied after the multi-head self-attention operation.\n",
    "        self.unifyheads = nn.Linear(k, k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, k = x.size()\n",
    "        h = self.heads\n",
    "\n",
    "        # 首先，为所有 heads 计算 query/key/value，得到的是完整嵌入维度的 k*k 矩阵\n",
    "        queries = self.toqueries(x)\n",
    "        keys = self.tokeys(x)\n",
    "        values = self.tovalues(x)\n",
    "\n",
    "        # 接下来将 queries/keys/values 切块（降维），分别送到不同的 head\n",
    "        s = k // h\n",
    "        keys = keys.view(b, t, h, s)\n",
    "        queries = queries.view(b, t, h, s)\n",
    "        values = values.view(b, t, h, s)\n",
    "\n",
    "        # - fold heads into the batch dimension\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "        values = values.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "\n",
    "        # Get dot product of queries and keys, and scale\n",
    "        dot = torch.bmm(\n",
    "            queries, keys.transpose(1, 2)\n",
    "        )  # -- dot has size (b*h, t, t) containing raw weights\n",
    "        dot = dot / (k ** (1 / 2))  # scale the dot product\n",
    "\n",
    "        # masking 操作，禁用矩阵对角线及其以下的元素，确保在预测时不会看到未来的信息\n",
    "        indices = torch.triu_indices(t, t, offset=1)\n",
    "        dot[:, indices[0], indices[1]] = float(\"-inf\")\n",
    "\n",
    "        dot = F.softmax(\n",
    "            dot, dim=2\n",
    "        )  # normalize, dot now contains row-wise normalized weights\n",
    "\n",
    "        out = torch.bmm(dot, values).view(\n",
    "            b, h, t, s\n",
    "        )  # apply the self attention to the values\n",
    "\n",
    "        # swap h, t back, unify heads\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t, s * h)\n",
    "\n",
    "        return self.unifyheads(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, k, heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = SelfAttention(k, heads=heads)\n",
    "        self.norm1 = nn.LayerNorm(k)\n",
    "        self.ff = nn.Sequential(nn.Linear(k, 4 * k), nn.ReLU(), nn.Linear(4 * k, k))\n",
    "        self.norm2 = nn.LayerNorm(k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attended = self.attention(x)\n",
    "        x = self.norm1(attended + x)\n",
    "\n",
    "        fedforward = self.ff(x)\n",
    "        return self.norm2(fedforward + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, k, heads, depth, seq_length, num_tokens, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "        self.token_emb = nn.Embedding(num_tokens, k)\n",
    "        self.pos_emb = nn.Embedding(seq_length, k)\n",
    "\n",
    "        # The sequence of transformer blocks that does all the heavy lifting\n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(TransformerBlock(k=k, heads=heads))\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "\n",
    "        # Maps the final output sequence to class logits\n",
    "        self.toprobs = nn.Linear(k, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: A (b, t) tensor of integer values representing words (in some predetermined vocabulary).\n",
    "        :return: A (b, c) tensor of log-probabilities over the classes (where c is the nr. of classes).\n",
    "        \"\"\"\n",
    "        # generate token embeddings\n",
    "        tokens = self.token_emb(x)\n",
    "        b, t, k = tokens.size()\n",
    "\n",
    "        # generate position embeddings\n",
    "        positions = torch.arange(t)\n",
    "        positions = positions.to(device)\n",
    "        positions = self.pos_emb(positions)[None, :, :].expand(b, t, k)\n",
    "\n",
    "        x = (\n",
    "            tokens + positions\n",
    "        )  # 为什么文本嵌入和位置嵌入相加，没有理论，可能就是实验下来效果不错。\n",
    "        # https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/\n",
    "        x = self.tblocks(x)  # shape: (batch_size, seq_length, embedding_dim)\n",
    "        # probabilities\n",
    "        # x = x.mean(dim=1)  # shape: (batch_size, embedding_dim)\n",
    "        x = self.toprobs(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    k=256,\n",
    "    heads=4,\n",
    "    depth=8,\n",
    "    seq_length=512,\n",
    "    num_tokens=vocab_size,\n",
    "    num_classes=vocab_size,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[\"<START>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, num_epochs, device, optimizer, criterion):\n",
    "    log_dict = {\n",
    "        \"train_loss_per_epoch\": [],\n",
    "        \"train_perplexity_per_epoch\": [],\n",
    "    }\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            if not batch_idx % 100:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch + 1:03d}/{num_epochs:03d} | Batch {batch_idx:05d}/{len(data_loader):05d} | Loss: {loss:.4f}\"\n",
    "                )\n",
    "        torch.save(model, \"model.pth\")\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader.dataset)\n",
    "        perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "        log_dict[\"train_loss_per_epoch\"].append(avg_loss)\n",
    "        log_dict[\"train_perplexity_per_epoch\"].append(perplexity)\n",
    "    return log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict =  train(model, data_loader, num_epochs, device, optimizer, criterion)\n",
    "# model = torch.load(\"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_stats(log_dict):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(log_dict[\"train_loss_per_epoch\"], label=\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(log_dict[\"train_perplexity_per_epoch\"], label=\"Training Perplexity\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.grid(True)\n",
    "    # plt.yscale(\"log\")\n",
    "    plt.title(\"Training Perplexity\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_training_stats(log_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(start_word=\"<START>\", top_k=1, log=False):\n",
    "    generated_text = \"\"\n",
    "    words = []\n",
    "    for word in start_word:\n",
    "        words += [word]\n",
    "    print(words)\n",
    "    with torch.no_grad():\n",
    "        vectors_list = []\n",
    "        for word in words:\n",
    "            word_vector = torch.LongTensor([word_to_index[word]]).unsqueeze(0)\n",
    "            vectors_list.append(word_vector)\n",
    "            generated_text += word\n",
    "\n",
    "        input_vector = torch.cat(vectors_list, dim=1)\n",
    "\n",
    "        for _ in range(context_len - len(words)):\n",
    "\n",
    "            output = model(input_vector.to(device))\n",
    "\n",
    "            last_word = output[:, -1, :]\n",
    "            last_word = last_word.view(-1)\n",
    "            top_values, top_indices = last_word.data.topk(top_k)\n",
    "\n",
    "            probabilities = top_values\n",
    "            top_words = [index_to_word[index.item()] for index in top_indices]\n",
    "\n",
    "            probabilities_np = probabilities.cpu().detach().numpy()\n",
    "            probabilities_np = probabilities_np / probabilities_np.sum()\n",
    "            indices_np = top_indices.cpu().detach().numpy()\n",
    "            if log:\n",
    "                for word, prob in zip(top_words, probabilities_np):\n",
    "                    print(f\"{word}: {prob:.4f}\")\n",
    "\n",
    "            selected_index = np.random.choice(indices_np, p=probabilities_np)\n",
    "\n",
    "            next_word = index_to_word[selected_index]\n",
    "            if next_word == \"<EOP>\":\n",
    "                break\n",
    "            generated_text += next_word\n",
    "            if log:\n",
    "                print(generated_text)\n",
    "            # * 需要升一个维, 因为模型的输入以 batch 为单位\n",
    "            input_vector = torch.cat(\n",
    "                [input_vector, torch.LongTensor([selected_index]).unsqueeze(0)], dim=1\n",
    "            )\n",
    "\n",
    "    return generated_text.strip()\n",
    "\n",
    "\n",
    "print(generate_text(\"长安一片月\", top_k=1))\n",
    "print(generate_text(\"故人西辞黄鹤楼\", top_k=1))\n",
    "print(generate_text(\"泉\", top_k=3))\n",
    "print(generate_text(\"日\", top_k=5))\n",
    "print(generate_text(\"沾衣欲湿杏花雨\", top_k=3))\n",
    "print(generate_text(\"风\", top_k=3, log=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
